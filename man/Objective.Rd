% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Objective.R
\docType{data}
\name{Objective}
\alias{Objective}
\title{Black-Box objective function}
\format{\link[R6:R6Class]{R6::R6Class} object.}
\description{
Specifies a black-box function and archive for optimizers to
act upon.
It allows the basic operations of querying the objective
at design points (\code{$eval_batch()}), storing the evaluations in an internal archive
and querying the archive (\code{$archive()}).

Evaluations of points are performed in batches.
Before and after a batch is evaluated, the \link{Terminator} is queried for the remaining budget.
If the available budget is exhausted, an exception is raised, and no further evaluations can be performed from this point on.

The optimizer is also supposed to store its final result, consisting of a list of optimal values
and stimated performance values, by calling the method \code{instance$assign_result}.
}
\section{Construction}{
\preformatted{inst = Objective$new param_set, terminator)
}

This defines the resampled performance of a learner on a task, a feasibility region
for the parameters the tuner is supposed to optimize, and a termination criterion.
\itemize{
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}.
\item \code{terminator} :: \link{Terminator}.
}
}

\section{Fields}{

\itemize{
\item \code{param_set} :: \link[paradox:ParamSet]{paradox::ParamSet}; from construction.
\item \code{terminator} :: \link{Terminator}; from construction.
\item \code{n_evals} :: \code{integer(1)}\cr
Number of configuration evaluations stored in the container.
\item \code{start_time} :: \code{POSIXct(1)}\cr
Time the tuning was started.
This is set in the beginning of \code{$tune()} of \link{Tuner}.
\item \code{result} :: named \code{list()}\cr
Result of the tuning, i.e., the optimal configuration and its estimated performance:
\itemize{
\item \code{"perf"}: Named vector of estimated performance values of the best configuration found.
\item \code{"tune_x"}: Named list of optimal hyperparameter settings, without potential \code{trafo} function applied.
\item \code{"params"}: Named list of optimal hyperparameter settings, similar to \code{tune_x}, but with potential \code{trafo} function applied.
Also, if the learner had some extra parameters statically set before tuning, these are included here.
}
}
}

\section{Methods}{

\itemize{
\item \code{eval_batch(dt)}\cr
\code{\link[data.table:data.table]{data.table::data.table()}} -> named \code{list()}\cr
Evaluates all hyperparameter configurations in \code{dt} through resampling, where each configuration is a row, and columns are scalar parameters.
Updates the internal \link{BenchmarkResult} \code{$bmr} by reference, and returns a named list with the following elements:
\itemize{
\item \code{"batch_nr"}: Number of the new batch.
This number is calculated in an auto-increment fashion and also stored inside the \link{BenchmarkResult} as column \code{batch_nr}
\item \code{"perf"}: A \code{\link[data.table:data.table]{data.table::data.table()}} of evaluated performances for each row of the \code{dt}.
Has the same number of rows as \code{dt}, and the same number of columns as length of \code{measures}.
Columns are named with measure-IDs. A cell entry is the (aggregated) performance of that configuration for that measure.
}

Before and after each batch-evaluation, the \link{Terminator} is checked, and if it is positive, an exception of class \code{terminated_error} is raised.
This function should be internally called by the tuner.
\item \code{best(measure = NULL)}\cr
(\link[mlr3:Measure]{mlr3::Measure}, \code{character(1)}) -> \link[mlr3:ResampleResult]{mlr3::ResampleResult}\cr
Queries the \link[mlr3:BenchmarkResult]{mlr3::BenchmarkResult} for the best \link[mlr3:ResampleResult]{mlr3::ResampleResult} according to \code{measure} (default is the first measure in \code{$measures}).
In case of ties, one of the tied values is selected randomly.
\item \code{archive(unnest = "no")}\cr
\code{character(1)} -> \code{\link[data.table:data.table]{data.table::data.table()}}\cr
Returns a table of contained resample results, similar to the one returned by \code{\link[mlr3:benchmark]{mlr3::benchmark()}}'s \code{$aggregate()} method.
Some interesting columns of this table are:
\itemize{
\item All evaluated measures are included as numeric columns, named with their measure ID.
\item \code{tune_x}: A list column that contains the parameter settings the tuner evaluated, without potential \code{trafo} applied.
\item \code{params}: A list column that contains the parameter settings that were actually used in the learner.
Similar to \code{tune_x}, but with potential \code{trafo} applied.
Also, if the learner had some extra parameters statically set before tuning, these are included here.
\code{unnest} can have the values \code{"no"}, \code{"tune_x"} or \code{"params"}. If it is not set to \code{"no"}, settings of the respective list-column
are stored in separate columns instead of the list-column, and dependent, inactive parameters are encoded with \code{NA}.
}
\item \code{assign_result(tune_x, perf)}\cr
(\code{list}, \code{numeric}) -> \code{NULL}\cr
The tuner writes the best found list of settings and estimated performance values here. For internal use.
\itemize{
\item \code{tune_x}: Must be a named list of settings only of parameters from \code{param_set} and be feasible, untransformed.
\item \code{perf} : Must be a named numeric vector of performance measures, named with performance IDs, regarding all elements in \code{measures}.
}
}
}

\keyword{datasets}
