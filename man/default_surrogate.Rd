% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mbo_defaults.R
\name{default_surrogate}
\alias{default_surrogate}
\title{Generate default surrogate}
\usage{
default_surrogate(instance, learner = NULL, n_objectives = NULL)
}
\arguments{
\item{instance}{\link[bbotk:OptimInstance]{bbotk::OptimInstance} \cr
An object that inherits from \link[bbotk:OptimInstance]{bbotk::OptimInstance}.}

\item{learner}{\link[mlr3:LearnerRegr]{mlr3::LearnerRegr} \cr
The surrogate learner used in the acquisition function. Defaults to \code{\link{default_surrogate}}.}
}
\value{
\link{\code{Learner}}
}
\description{
This is a helper function that generates a default surrogate, based on properties of the objective
function and the selected infill criterion.

For numeric-only (including integers) parameter spaces without any dependencies:
\itemize{
\item{A Kriging model \dQuote{regr.km} with kernel \dQuote{matern3_2} is created.}
\item{If the objective function is deterministic we add a small nugget effect (10^-8*Var(y),
y is vector of observed outcomes in current design) to increase numerical stability to
hopefully prevent crashes of DiceKriging.
Whether the objective function is deterministic can be observed from the objective functions
properties.}
\item{If the objective function is noisy the nugget effect will be estimated with
\code{nugget.estim = TRUE}}
Also \code{jitter} is set to \code{TRUE} to circumvent a problem with DiceKriging where already
trained input values produce the exact trained output.
\item{Instead of the default \code{"BFGS"} optimization method we use rgenoud (\code{"gen"}),
which is a hybrid algorithm, to combine global search based on genetic algorithms and local search
based on gradients.
This may improve the model fit and will less frequently produce a constant surrogate model.}
}

For mixed numeric-categorical parameter spaces, or spaces with conditional parameters:
\itemize{
\item{A random regression forest \dQuote{regr.randomForest} with 500 trees is created.}
\item{The standard error of a prediction (if required by the infill criterion) is estimated
by computing the jackknife-after-bootstrap.
This is the \code{se.method = "jackknife"} option of the \dQuote{regr.randomForest} Learner.
}
}
In any case, learners are encapsulated using "evaluate", and a fallback learner is set, in cases
where the surrogate learner errors. Currently, the following learner is used as a fallback:
\code{ GraphLearner$new(po("imputeoor") \%>>\% lrn("regr.ranger", num.trees = 20L, keep.inbag = TRUE))}.

If additionally dependencies are present in the parameter space, inactive conditional parameters
are represented by missing \code{NA} values in the training design data.frame.
We simply handle those with an imputation method, added to the random forest, more concretely we
use \code{po("imputeoor")} from package \CRANpkg{mlr3pipelines}.
Both of these techniques make sense for tree-based methods and are usually hard to beat, see
Ding et.al. (2010).
}
\references{
Ding, Yufeng, and Jeffrey S. Simonoff. An investigation of missing data methods for
classification trees applied to binary response data.
Journal of Machine Learning Research 11.Jan (2010): 131-170.
}
\seealso{
Other mbo_defaults: 
\code{\link{default_acq_optimizer}()},
\code{\link{default_acqfun}()},
\code{\link{default_loopfun}()},
\code{\link{mbo_defaults}}
}
\concept{mbo_defaults}
