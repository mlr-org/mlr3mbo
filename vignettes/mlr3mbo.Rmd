---
title: "mlr3mbo"
subtitle: "Flexible Bayesian Optimization in R"
output: rmarkdown::html_vignette
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{mlr3mbo}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

update_db = function() {
  if (is.null(db$base) || is.null(db$aliases)) {
    hdb = hsearch_db(package = unique(c(db$index, db$hosted)), types = "help")
    db$base = setkeyv(as.data.table(hdb$Base), "ID")
    db$aliases = setkeyv(as.data.table(hdb$Aliases), "Alias")
  }
}

#' @title Hyperlink to Function Reference
#'
#' @description
#' Creates a markdown link to a function reference.
#'
#' @param topic Name of the topic to link against.
#' @param text Text to use for the link. Defaults to the topic name.
#' @param format Either markdown or HTML.
#'
#' @return (`character(1)`) markdown link.
ref = function(topic, text = topic, format = "markdown") {
  strip_parenthesis = function(x) sub("\\(\\)$", "", x)

  checkmate::assert_string(topic, pattern = "^[[:alnum:]._-]+(::[[:alnum:]._-]+)?(\\(\\))?$")
  checkmate::assert_string(text, min.chars = 1L)
  checkmate::assert_choice(format, c("markdown", "html"))

  topic = trimws(topic)
  text = trimws(text)

  if (stringi::stri_detect_fixed(topic, "::")) {
    parts = strsplit(topic, "::", fixed = TRUE)[[1L]]
    topic = parts[2L]
    name = strip_parenthesis(parts[2L])
    pkg = parts[1L]
  } else {
    update_db()
    matched = db$base[db$aliases[list(strip_parenthesis(topic)), c("Alias", "ID"), on = "Alias", nomatch = 0L], on = "ID", nomatch = NULL]
    if (nrow(matched) == 0L) {
      stop(sprintf("Could not find help page for topic '%s'", topic))
    }
    if (nrow(matched) >= 2L) {
      lgr$warn("Ambiguous link to '%s': %s", topic, paste0(paste(matched$Package, matched$Name, sep = "::"), collapse = " | "))
      matched = head(matched, 1L)
    }

    pkg = matched$Package
    name = matched$Name
    lgr$debug("Resolved '%s' to '%s::%s'", topic, pkg, name)
  }

  if (pkg %in% db$hosted) {
    url = sprintf("https://%s.mlr-org.com/reference/%s.html", pkg, name)
  } else {
    url = sprintf("https://www.rdocumentation.org/packages/%s/topics/%s", pkg, name)
  }

  switch(format,
    "markdown" = sprintf("[`%s`](%s)", text, url),
    "html" = sprintf("<a href=\"%s\">%s</a>", url, text)
  )
}

#' @title Hyperlink to Package
#'
#' @description
#' Links either to respective mlr3 website or to CRAN page.
#'
#' @param pkg Name of the package.
#' @inheritParams ref
#'
#' @return (`character(1)`) markdown link.
#' @export
ref_pkg = function(pkg, format = "markdown") {
  checkmate::assert_string(pkg, pattern = "^[[:alnum:]._-]+$")
  checkmate::assert_choice(format, c("markdown", "html"))
  pkg = trimws(pkg)

  if (grepl("/", pkg, fixed = TRUE)) {
    gh_pkg(pkg, format = format)
  } else if (pkg %in% db$hosted) {
    mlr_pkg(pkg, format = format)
  } else {
    cran_pkg(pkg, format = format)
  }
}

#' @title Hyperlink to CRAN Package
#'
#' @description
#' Creates a markdown link to a CRAN package.
#'
#' @inheritParams ref_pkg
#'
#' @return (`character(1)`) markdown link.
cran_pkg = function(pkg, format = "markdown") {
  checkmate::assert_string(pkg, pattern = "^[[:alnum:]._-]+$")
  checkmate::assert_choice(format, c("markdown", "html"))
  pkg = trimws(pkg)

  if (pkg %in% c("stats", "graphics", "datasets")) {
    return(pkg)
  }
  url = sprintf("https://cran.r-project.org/package=%s", pkg)
  switch(format,
    "markdown" = sprintf("[%s](%s)", pkg, url),
    "html" = sprintf("<a href = \"%s\">%s</a>", url, pkg)
  )
}

#' @title Hyperlink to mlr3 Package
#'
#' @description
#' Creates a markdown link to a mlr3 package with a "mlr-org.com" subdomain.
#'
#' @inheritParams ref_pkg
#'
#' @return (`character(1)`) markdown link.
mlr_pkg = function(pkg, format = "markdown") {
  checkmate::assert_string(pkg, pattern = "^[[:alnum:]._-]+$")
  checkmate::assert_choice(format, c("markdown", "html"))
  pkg = trimws(pkg)

  url = sprintf("https://%1$s.mlr-org.com", pkg)
  switch(format,
    "markdown" = sprintf("[%s](%s)", pkg, url),
    "html" = sprintf("<a href = \"%s\">%s</a>", url, pkg)
  )
}

#' @title Hyperlink to GitHub Repository
#'
#' @description
#' Creates a markdown link to GitHub repository.
#'
#' @param pkg Name of the repository specified as "{repo}/{name}".
#' @inheritParams ref_pkg
#'
#' @return (`character(1)`) markdown link.
gh_pkg = function(pkg, format = "markdown") {
  checkmate::assert_string(pkg, pattern = "^[[:alnum:]_-]+/[[:alnum:]._-]+$")
  checkmate::assert_choice(format, c("markdown", "html"))
  pkg = trimws(pkg)

  parts = strsplit(pkg, "/", fixed = TRUE)[[1L]]
  url = sprintf("https://github.com/%s", pkg)
  switch(format,
    "markdown" = sprintf("[%s](%s)", parts[2L], url),
    "html" = sprintf("<a href = \"%s\">%s</a>", url, parts[2L])
  )
}

db = new.env()
db$index = c("base", "utils", "datasets", "data.table", "stats")
db$hosted = c("paradox", "mlr3misc", "mlr3", "mlr3data", "mlr3db", "mlr3proba", "mlr3pipelines", "mlr3learners", "mlr3filters", "bbotk", "mlr3tuning", "mlr3viz", "mlr3fselect", "mlr3cluster", "mlr3spatiotempcv", "mlr3spatial", "mlr3extralearners", "mlr3tuningspaces", "mlr3hyperband", "mlr3mbo")

lgr = NULL
```

# Intro

`r ref_pkg("mlr3mbo")` makes model Bayesian optimization (BO) available within the `r ref_pkg("mlr3")` ecosystem.
BO can be used for optimizing any black box function, and is very suitable for hyperparameter optimization of machine learning models.
`r ref_pkg("mlr3mbo")` allows for building custom BO algorithms relying on building blocks in a modular fashion, but also provides a variety of standard single- and multi-objective BO algorithms that can be used in a straightforward manner.

We assume that the reader is somewhat familiar with black box optimization and `r ref_pkg("bbotk")`, hyperparameter optimization and `r ref_pkg("mlr3tuning")` and knows the basics of BO.
Background material is, for example, given by @garnett_2022, @bischl_2021, and @mlr3book.

# Building Blocks

BO is an iterative optimization algorithm that makes use of a so-called surrogate to model the unknown black box function.
After having observed an initial design of observations, the surrogate model is trained on all data points observed so far and an acquisition function is used to determine which points of the search space are promising candidates that should be evaluated next.
The acquisition function relies only on the prediction of the surrogate model and requires no evaluation of the true black box function and therefore is comparably cheap to optimize.
After having evaluated the next candidate, the process repeats itself until a given termination criteria is met.

Most BO flavors therefore follow a simple loop:
  1. Fit the surrogate on all observations made so far.
  2. Optimize the acquisition function to find the next candidate that should be evaluated.
  3. Evaluate the next candidate.

In the following, the basic building blocks of BO and their implementation in `r ref_pkg("mlr3mbo")` are introduced in more detail.

## Loop Function

The `r ref("mlr3mbo::loop_function", "loop_function")` determines the behavior of the BO algorithm on a global level, i.e., how the subroutine should look like that is performed at each iteration.

To get an overview of readily available `loop_function`s, the following dictionary can be inspected:

```{r}
library(mlr3mbo)
library(data.table)
as.data.table(mlr_loop_functions)
```

The dictionary shows the `key`, i.e., ID of the `loop_function`, a brief description, for which optimization instance the resulting BO flavor can be used (i..e, `OptimInstanceSingleCrit` for
single-objective and `OptimInstanceMultiCrit` for multi-objective optimization) as well as how documentation can be accessed.

Technically, all `loop_function`s are members of the `S3` class `r ref( "mlr3mbo::loop_function", "loop_function")`, which are simply decorated `r ref("base::function", "functions")` (because using an `r ref("R6::R6Class", "R6Class")` class would be over the top here - but this may change in the future).

To write an own `loop_function`, users can get inspiration from the readily available ones, i.e.,
`r ref("mlr3mbo::bayesopt_ego", "bayesopt_ego")` which performs sequential single-objective optimization:

After having made some assertions and safety checks, and having evaluated the initial design `bayesopt_ego` essentially only performs the following steps:

1. `acq_function$surrogate$update()`  # update the surrogate model
2. `acq_function$update()`  # update the acquisition function (i.e., update the best point observed so far)
3. `acq_optimizer$optimize()`  # optimize the acquisition function to yield a new candidate

## Surrogate

A surrogate encapsulates a regression learner that models the unknown black box function based on observed data.
In `r ref_pkg("mlr3mbo")`, `r ref("mlr3mbo::SurrogateLearner", "SurrogateLearner")` and `r ref( "mlr3mbo::SurrogateLearnerCollection", "SurrogateLearnerCollection")` are the higher-level `R6` classes which should be used to construct a surrogate, inheriting from the base `r ref("mlr3mbo::Surrogate", "Surrogate")` class.

As a learner, any `r ref("mlr3::LearnerRegr", "LearnerRegr")` from `r ref_pkg("mlr3")` can be used, however, most acquisition functions require both a mean and a variance prediction (therefore not all learners are suitable for all scenarios).
Typical choices include:

* A `r ref("mlr3learners::LearnerRegrKM", "Gaussian process")` for low dimensional numeric search spaces
* A `r ref("mlr3learners::LearnerRegrRanger", "Random forest")` for higher dimensional mixed (or hierarchical) search spaces

A `SurrogateLearner` can be constructed via

```{r}
library(mlr3learners)
surrogate = SurrogateLearner$new(lrn("regr.km"))
```

or

```{r}
surrogate = srlrn(lrn("regr.km"))
```

The encapsulated learner can be accessed via the `$model` field:

```{r}
surrogate$model
```


The surrogate itself has the following hyperparameters:

```{r}
surrogate$param_set
```

`assert_insample_perf = TRUE` results in the insample performance of the learner being calculated after each `$update()`.
This requires the specification of a `perf_measure` (any regression measure, e.g., `r ref("mlr3::mlr_measures_regr.rsq", "R squared")`) and a `perf_threshold`.
After fitting the surrogate, the insample performance of the surrogate is calculated and asserted against the performance threshold.
If the threshold is not met, an error is thrown (that is caught within the optimization loop - unless `catch_errors = FALSE` and results in, e.g., proposing the next candidate uniformly at random).
For more details on this mechanism, see the [Safety Nets](#safety-nets) section.

Note that this assertion is not always meaningful, e.g., in the case of using a Gaussian process with no nugget, the insample performance will always be perfect.

Internally, the learner is fitted on a regression task constructed from the `r ref("bbotk::Archive", "Archive")` of the `r ref("bbotk::OptimInstance", "OptimInstance")` that is to be optimized and features and the target variable are determined automatically but can also be specified via the `x_cols` and `y_cols` active bindings.
Ideally, the `Archive` is already passed during construction, however, lazy initialization is also possible.

Important methods are `$update()` and `$predict()` with the former one typically being used within the `loop_function` and the latter one being used within the evaluation of an acquisition function.

Depending on the choice of the `loop_function`, multiple targets must be modelled by (currently independent) surrogates, in which case a `SurrogateLearnerCollection` should be used.
Construction and hyperparameters are analogous to the single target scenario described above.

## Acquisition Function

Based on a surrogate, an acquisition function quantifies the attractiveness of each point in the search space if it was to be evaluated in the next iteration.

A popular example is given by the Expected Improvement [@jones_1998]:

$$
\mathbb{E}_{y} \left[ \max \left( f_{\mathrm{min}} - y, 0 \right) \right],
$$
here $y$ is the surrogate prediction for a given point $x$ (which when using a Gaussian process as surrogate
follows a normal distribution) and $f_{\mathrm{min}}$ is the currently best function value observed so far.

To get an overview of available acquisition functions, the following dictionary can be inspected:

```{r}
as.data.table(mlr_acqfunctions)
```

The dictionary shows the `key`, i.e., ID of the acquisition function, a brief description, and how the documentation can be accessed.

Technically, all acquisition functions inherit from the `R6` class `r ref( "mlr3mbo::AcqFunction", "AcqFunction")` which itself simply inherits from the base `r ref("bbotk::Objective", "Objective")` class.

Construction is straightforward via 

```{r}
acq_function = AcqFunctionEI$new()
```

or using syntactic sugar

```{r}
acq_function = acqf("ei")
```

Internally, the `r ref("paradox::Domain", "Domain")` and `r ref("paradox::Codomain", "Codomain")` are constructed based on the `r ref("bbotk::Archive", "Archive")` referenced by the `r ref("mlr3mbo::Surrogate", "Surrogate")` and therefore the surrogate should be passed as an argument already during construction.

However, lazy initialization is also possible.

In the case of the acquisition function itself being parameterized, hyperparameters should be passed as constants, e.g.:

```{r}
acqf("cb")  # lower / upper confidence bound with lambda hyperparameter
```

## Acquisition Function Optimizer

To find the most promising candidate for evaluation, the acquisition function itself must be optimized.
Internally, an `r ref("bbotk::OptimInstance", "OptimInstance")` is constructed using the acquisition function as an `r ref("bbotk::Objective", "Objective")` and the domain and codomain of the acquisition function are used.

An acquisition function optimizer is then used to solve this optimization problem.
Technically, this optimizer is a member of the `r ref("mlr3mbo::AcqOptimizer", "AcqOptimizer")` `R6` class.

Construction requires specifying an `r ref("bbotk::Optimizer", "Optimizer")` as well as a `r ref( "bbotk::Terminator", "Terminator")`:

```{r}
library(bbotk)
acq_optimizer = AcqOptimizer$new(opt("random_search"), terminator = trm("evals"))
```

```{r}
acq_optimizer = acqo(opt("random_search"), terminator = trm("evals"))
```

The optimizer and terminator can be accessed via the `$optimizer` and `$terminator` fields:

```{r}
acq_optimizer$optimizer
acq_optimizer$terminator
```

Internally, the acquisition function optimizer also requires the acquisition function and therefore the `acq_function` argument should be specified during construction.

However, lazy initialization is also possible.

Any `r ref("mlr3mbo::AcqOptimizer", "AcqOptimizer")` has the following hyperparameters

```{r}
acq_optimizer$param_set
```

`catch_errors = TRUE` results in catching any errors that can happen during the acquisition function optimization which allows for, e.g., proposing the next candidate uniformly at random within the `loop_function`.
For more details on this mechanism, see the [Safety Nets](#safety-nets) section.
`logging_level` specifies the logging level during acquisition function optimization.
Often it is useful to only log the progress of the BO loop and therefore `logging_level` is set to `"warn"` by default.
For debugging purposes, this should be set to `"info"`.
`skip_already = TRUE` will result in not proposing candidates for evaluation that were already evaluated in previous iterations.
`warmstart = TRUE` results in the best `warmstart_size` points present in the `Archive` of the `OptimInstance` to also be evaluated on the acquisition function `OptimInstance` prior to running the actual optimization.
This is especially useful in the case of using evolutionary algorithms or variants of local search as the acquisition function optimizer (as the current best points should usually be part of the initial population to further optimize local optima).

# Putting it Together

Having introduced all building blocks we are now ready to put everything together in the form of an `r ref( "mlr3mbo::OptimizerMbo", "OptimizerMbo")` and `r ref("mlr3mbo::TunerMbo", "TunerMbo")`.

`OptimizerMbo` inherits from `r ref("bbotk::Optimizer", "Optimizer")` and requires a `loop_function`, `surrogate`, `acq_function` and `acq_optimizer`.

Besides, additional arguments, i.e., arguments of the `loop_function` can be passed via the `args` active binding.
Finally, the mechanism how the final result is obtained after the optimization progress (i.e., the best point in the case of single-objective and the Pareto set in the case of multi-objective optimization) can be changed via the `result_function` active binding.
As an example, `r ref("mlr3mbo::result_by_surrogate_design", "result_by_surrogate_design")` will choose the final solution based on the prediction of the surrogate instead of the evaluations logged in the `Archive` which is sensible in the case of noisy objective functions.

Construction is performed via

```{r}
optimizer = OptimizerMbo$new(bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)
```

or using syntactic sugar

```{r}
optimizer = opt("mbo", bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)
```

Note that important fields such as `$param_classes`, `$packages`, `$properties` are automatically determined based on the choice of the `loop_function`, `surrogate`, `acq_function`, and `acq_optimizer`.

If arguments such as the `surrogate`, `acq_function`, and `acq_optimizer` where not fully specified during construction, i.e., the `surrogate` missing the `archive`, or the `acq_function` missing the `surrogate`, lazy initialization is completed prior to the optimizer being used for optimization.

An object of class `OptimizerMbo` can be used to optimize an object of class `r ref("bbotk::OptimInstanceSingleCrit", "OptimInstanceSingleCrit")` or `r ref( "bbotk::OptimInstanceMulticrit", "OptimInstanceMultiCrit")`.

For hyperparameter optimization, `r ref("mlr3mbo::TunerMbo", "TunerMbo")` should be used (which simply relies on an `OptimizerMbo` that is constructed internally):

```{r}
tuner = TunerMbo$new(bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)
mlr3misc::get_private(tuner)[[".optimizer"]]
# tuner = tnr("mbo", bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)
```

## The Initial Design

`r ref_pkg("mlr3mbo")` offers two different ways for specifying an initial design:

1. One can simply evaluate points on the `OptimInstance` that is to be optimized prior to using an `OptimizerMbo`. In this case, the `loop_function` should skip the construction and evaluation of an initial design.
2. If no points were already evaluated on the `OptimInstance`, the `loop_function` should construct an initial design and evaluate it, e.g., `bayesopt_ego` constructs an initial design of size $4D$ where $D$ is the dimensionality of the search space sampling points uniformly at random.

# Defaults

`r ref_pkg("mlr3mbo")` tries to use intelligent defaults for the `loop_function`, `surrogate`, `acq_function`, and `acq_optimizer` within `OptimizerMbo` and `TunerMbo`.

For details, see `r ref("mlr3mbo::mbo_defaults", "mbo_defaults")`.

# Safety Nets

`r ref_pkg("mlr3mbo")` is quite stable in the sense that - if desired - all kinds of errors can be caught and handled appropriately within the `r ref("mlr3mbo::loop_function", "loop_function")`.

As an example, let's have a look at the inner workings of `r ref("mlr3mbo::bayesopt_ego", "bayesopt_ego")`:

```{r, eval = FALSE}
  repeat {
    xdt = tryCatch({
      # random interleaving is handled here
      if (isTRUE((instance$archive$n_evals - init_design_size + 1L) %% random_interleave_iter == 0)) {
        stop(set_class(list(message = "Random interleaving", call = NULL), classes = c("mbo_error", "random_interleave", "error", "condition")))
      }
      acq_function$surrogate$update()
      acq_function$update()
      acq_optimizer$optimize()
    }, mbo_error = function(mbo_error_condition) {
      lg$info(paste0(class(mbo_error_condition), collapse = " / "))
      lg$info("Proposing a randomly sampled point")
      SamplerUnif$new(domain)$sample(1L)$data
    })

    instance$eval_batch(xdt)
    if (instance$is_terminated) break
  }
```

In each iteration, a new candidate is chosen based on updating the surrogate, and optimizing the acquisition function.
If any error happens during any of these steps, errors are upgraded to errors of class `"mbo_error"` (and `"surrogate_update_error"` for surrogate related errors as well as `"acq_optimizer_error"` for acquisition function optimization related errors).
These errors are then caught and a fallback is triggered: Evaluating the next candidate chosen uniformly at random.
Note that the same mechanism is actually also used to handle random interleaving.

To illustrate this, consider the following scenario:
We try to optimize a function, however, our Gaussian process fails due to data points being too close to each other.

```{r}
domain = ps(x = p_dbl(lower = -1, upper = 1))

codomain = ps(y = p_dbl(tags = "minimize"))

objective_function = function(xs) {
  list(y = as.numeric(xs)^2)
}

objective = ObjectiveRFun$new(
 fun = objective_function,
 domain = domain,
 codomain = codomain)

instance = OptimInstanceSingleCrit$new(
 objective = objective,
 terminator = trm("evals", n_evals = 10))

initial_design = data.table(x = rep(0, 4))
instance$eval_batch(initial_design)

surrogate = srlrn(lrn("regr.km", covtype = "matern3_2", optim.method = "gen", nugget.stability = 10^-8, control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("random_search", batch_size = 1000), terminator = trm("evals", n_evals = 1000))
optimizer = opt("mbo", loop_function = bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)

optimizer$optimize(instance)
```

The log tells us that an error happened and was caught: `"mbo_error / surrogate_update_error / error / condition"`.
We also see in the `Archive` that the first candidate after the initial design (the fifth point) was not proposed based on optimizing the acquisition function:

```{r}
instance$archive$data
```

Nevertheless, due to the safety net, the BO loop eventually worked just fine and did not simply throw an error.

If we set `catch_errors = FALSE` within the surrogate, we see that the error was indeed caused by the surrogate:

```{r, error = TRUE}
instance$archive$clear()
instance$eval_batch(initial_design)
optimizer$surrogate$param_set$values$catch_errors = FALSE
optimizer$optimize(instance)
```

In case of the error belonging to the `acq_optimizer_error` class, it is helpful to increase the logging level of the acquisition function optimizer (`acq_optimizer$param_set$params$logging_level`) and also set `catch_errors = FALSE`.

This allows for straightforward debugging.

To make sure that your BO loop behaved as expected, always inspect the log of the optimization process and inspect the `Archive` and check whether the acquisition function column is populated as expected.

# Examples

In this final section, some standard examples are provided.

## Single-Objective: 2D Schwefel Function

```{r, eval=FALSE}
objective_function = function(xs) {
  list(y = 418.9829 * 2 - (sum(unlist(xs) * sin(sqrt(abs(unlist(xs)))))))
}
domain = ps(x1 = p_dbl(lower = - 500, upper = 500), x2 = p_dbl(lower = -500, upper = 500))
codomain = ps(y = p_dbl(tags = "minimize"))

objective = ObjectiveRFun$new(
 fun = objective_function,
 domain = domain,
 codomain = codomain)

instance = OptimInstanceSingleCrit$new(
 objective = objective,
 search_space = domain,
 terminator = trm("evals", n_evals = 60))

# Gaussian process, EI, DIRECT
surrogate = srlrn(lrn("regr.km", covtype = "matern3_2", optim.method = "gen", nugget.stability = 10^-8, control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_DIRECT_L"), terminator = trm("stagnation", threshold = 1e-8))
optimizer = opt("mbo", loop_function = bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)

set.seed(2906)
optimizer$optimize(instance)
```

```{r, eval=FALSE}
library(ggplot2)

ggplot(aes(x = batch_nr, y = cummin(y)), data = instance$archive$data) +
  geom_point() +
  geom_step() +
  labs(x = "Batch Nr.", y = "Best y") +
  theme_minimal()
```

```{r, eval=FALSE}
xdt = generate_design_grid(instance$search_space, resolution = 101)$data
ydt = objective$eval_dt(xdt)
ggplot(aes(x = x1, y = x2, z = y), data = cbind(xdt, ydt)) +
  geom_contour_filled() +
  geom_point(aes(x = x1, y = x2, color = batch_nr), size = 2, data = instance$archive$data) +
  scale_color_gradient(low = "lightgrey", high = "red") +
  theme_minimal()
```

## Multi-Objective: Schaffer Function N. 1

### ParEGO

```{r, eval=FALSE}
objective_function = function(xs) {
  list(y1 = xs$x ^ 2, y2 = (xs$x - 2) ^ 2)
}
domain = ps(x = p_dbl(lower = -10, upper = 10))
codomain = ps(y1 = p_dbl(tags = "minimize"), y2 = p_dbl(tags = "minimize"))

objective = ObjectiveRFun$new(
 fun = objective_function,
 domain = domain,
 codomain = codomain)

instance = OptimInstanceMultiCrit$new(
 objective = objective,
 search_space = domain,
 terminator = trm("evals", n_evals = 30))

# Gaussian process, EI, DIRECT
surrogate = srlrn(lrn("regr.km", covtype = "matern3_2", optim.method = "gen", nugget.stability = 10^-8, control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_DIRECT_L"), terminator = trm("stagnation", threshold = 1e-8))
optimizer = opt("mbo", loop_function = bayesopt_parego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)

set.seed(2906)
optimizer$optimize(instance)
```

```{r, eval=FALSE}
ggplot(aes(x = y1, y = y2), data = instance$archive$best()) +
  geom_point() +
  theme_minimal()
```

```{r, eval=FALSE}
anytime_hypervolume = map_dtr(unique(instance$archive$data$batch_nr), function(bnr) {
  dhv = emoa::dominated_hypervolume(t(instance$archive$best(batch = 1:bnr)[, instance$archive$cols_y, with = FALSE]), ref = t(t(c(100, 144))) )
  data.table(batch_nr = bnr, dhv = dhv)
})

ggplot(aes(x = batch_nr, y = dhv), data = anytime_hypervolume[batch_nr > 1]) +
  geom_point() +
  geom_step(direction = "vh") +
  labs(x = "Batch Nr.", y = "Dominated Hypervolume") +
  theme_minimal()
```

### SmsEGO

```{r, eval=FALSE}
objective_function = function(xs) {
  list(y1 = xs$x ^ 2, y2 = (xs$x - 2) ^ 2)
}
domain = ps(x = p_dbl(lower = -10, upper = 10))
codomain = ps(y1 = p_dbl(tags = "minimize"), y2 = p_dbl(tags = "minimize"))

objective = ObjectiveRFun$new(
 fun = objective_function,
 domain = domain,
 codomain = codomain)

instance = OptimInstanceMultiCrit$new(
 objective = objective,
 search_space = domain,
 terminator = trm("evals", n_evals = 30))

# Gaussian processes, SmsEGO, DIRECT
learner_y1 = lrn("regr.km", covtype = "matern3_2", optim.method = "gen", nugget.stability = 10^-8, control = list(trace = FALSE))
learner_y2 = learner_y1$clone(deep = TRUE)
surrogate = srlrnc(list(learner_y1, learner_y2))
acq_function = acqf("sms_ego")
acq_optimizer = acqo(opt("nloptr", algorithm = "NLOPT_GN_DIRECT_L"), terminator = trm("stagnation", threshold = 1e-8))
optimizer = opt("mbo", loop_function = bayesopt_smsego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)

set.seed(2906)
optimizer$optimize(instance)
```

```{r, eval=FALSE}
ggplot(aes(x = y1, y = y2), data = instance$archive$best()) +
  geom_point() +
  theme_minimal()
```

```{r, eval=FALSE}
anytime_hypervolume = map_dtr(unique(instance$archive$data$batch_nr), function(bnr) {
  dhv = emoa::dominated_hypervolume(t(instance$archive$best(batch = 1:bnr)[, instance$archive$cols_y, with = FALSE]), ref = t(t(c(100, 144))) )
  data.table(batch_nr = bnr, dhv = dhv)
})

ggplot(aes(x = batch_nr, y = dhv), data = anytime_hypervolume[batch_nr > 1]) +
  geom_point() +
  geom_step(direction = "vh") +
  labs(x = "Batch Nr.", y = "Dominated Hypervolume") +
  theme_minimal()
```

## Single-Objective HPO

```{r, eval=FALSE}
library(mlr3)
task = tsk("wine")
learner = lrn("classif.rpart", cp = to_tune(lower = 1e-4, upper = 1, logscale = TRUE), maxdepth = to_tune(lower = 1, upper = 30), minbucket = to_tune(lower = 1, upper = 100), minsplit = to_tune(lower = 1, upper = 100))
resampling = rsmp("cv", folds = 3)
measure = msr("classif.acc")

instance = TuningInstanceSingleCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measure = measure,
  terminator = trm("evals", n_evals = 30))
  
# Gaussian process, EI, FocusSearch
surrogate = srlrn(lrn("regr.km", covtype = "matern3_2", optim.method = "gen", nugget.estim = TRUE, jitter = 1e-12, control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("focus_search", n_points = 100L, maxit = 9), terminator = trm("evals", n_evals = 3000))
tuner = tnr("mbo", loop_function = bayesopt_ego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer)

set.seed(2906)
tuner$optimize(instance)
instance$result
instance$archive$best()  # result_function is result_by_surrogate_design
```

## Multi-Objective HPO

```{r, eval=FALSE}
task = tsk("wine")
learner = lrn("classif.rpart", cp = to_tune(lower = 1e-4, upper = 1, logscale = TRUE), maxdepth = to_tune(lower = 1, upper = 30), minbucket = to_tune(lower = 1, upper = 100), minsplit = to_tune(lower = 1, upper = 100))
resampling = rsmp("cv", folds = 3)
measures = msrs(c("classif.acc", "selected_features"))

instance = TuningInstanceMultiCrit$new(
  task = task,
  learner = learner,
  resampling = resampling,
  measures = measures,
  terminator = trm("evals", n_evals = 30),
  store_models = TRUE)  # required due to selected features
  
# Gaussian process, EI, FocusSearch
surrogate = srlrn(lrn("regr.km", covtype = "matern3_2", optim.method = "gen", nugget.estim = TRUE, jitter = 1e-12, control = list(trace = FALSE)))
acq_function = acqf("ei")
acq_optimizer = acqo(opt("focus_search", n_points = 100L, maxit = 9), terminator = trm("evals", n_evals = 3000))
# result_by_surrogate_design is by default not sensible for ParEGO
tuner = tnr("mbo", loop_function = bayesopt_parego, surrogate = surrogate, acq_function = acq_function, acq_optimizer = acq_optimizer, result_function = result_by_default)

set.seed(2906)
tuner$optimize(instance)
instance$result
```

